{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03a4c1e-6229-4b50-93e3-ca7cb17f7faf",
   "metadata": {},
   "source": [
    "## Student Name: \n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial 2 - Part II: Bayesian mixture modeling\n",
    "\n",
    "All the parts that require action (either in the code or equations) are flagged by `<your turn>` or $\\color{red}{<your~turn>}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321a791-1929-491b-9244-046d0b1b8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.color_palette(\"hls\", 8)\n",
    "\n",
    "# Setting matplotlib fonts\n",
    "from matplotlib import rc\n",
    "font = {'family': 'serif',\n",
    "        'weight': 'bold',\n",
    "        'size': '14'}\n",
    "rc('font', **font)\n",
    "\n",
    "\n",
    "# Below is a set of colors for matplotlib that is colorblind-friendly\n",
    "# To use them in plotting commands, you can simply set \"color=colorset[N]\",\n",
    "# where N is an integer in [0,16), reflecting the index of the colors below.\n",
    "colorset = ['#000000','#00270C','#00443C','#005083',\n",
    "            '#034BCA','#483CFC','#9C2BFF','#EB24F4',\n",
    "            '#FF2DC2','#FF4986','#FF7356','#FFA443',\n",
    "            '#EBD155','#D3F187','#D7FFC8','#FFFFFF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08ee47-0889-4a46-9690-95e822b3ecd8",
   "metadata": {},
   "source": [
    "## Mixture modeling\n",
    "\n",
    "Starting with the example in the lecture, we have made $N$ independent measurements of a quantity $Y$ (with each measurement containing normally-distributed uncertainties), at different values of quantity $X$. Assuming $Y$ as a function of $X$, we want to infer a model for the \"model\" $Y$ (which we label $Y_{\\rm{Model}}$), given what we can *observe* as $Y$ ($Y_{\\rm{obs}}$).\n",
    "\n",
    "### Data\n",
    "\n",
    "Our data consists of a sample a single quantity per measurement:\n",
    "\n",
    "\n",
    "$$ \\hat{X} = [\\hat{x}_1,\\cdots,\\hat{x}_N] $$\n",
    "\n",
    "\n",
    "In this unit we will use [Pandas](https://pandas.pydata.org/) for data operations. Here is [a quick cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) for using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4414bd4-0c6d-44f3-bc98-31a65c9e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('https://raw.githubusercontent.com/bersavosh/P4003/refs/heads/main/Tutorials/T2_data_mixture.csv')\n",
    "DATA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805e4de-dcd0-4acd-81af-50a95af78b67",
   "metadata": {},
   "source": [
    "### EDA\n",
    "#### Exercise: Make a histogram and a kde plot the measurements using [Seaborn](https://seaborn.pydata.org/tutorial/distributions.html). Include a rug visualization in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f8120-ade2-403e-8f40-fb25a786f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3888fe3-d3f1-4c9b-b017-203b26eb8885",
   "metadata": {},
   "source": [
    "### Our model\n",
    "\n",
    "Similar to the lecture, we consider that our observed values of X are random draws from a *mixture* of two gaussians:\n",
    "$$ \\hat{X}\\sim \\rm{Mixture}\\{\\mathcal{N}_1(\\mu_1, \\sigma_1),\\mathcal{N}_2(\\mu_2, \\sigma_2)\\} $$\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "$$ p(\\hat{X} |\\mu_1,\\sigma_1,w_1,\\mu_2,\\sigma_2,w_2) = \\prod_{i=1}^{N}\\left[\\sum_{j=1}^{2} w_j \\mathcal{N}(\\mu_j,\\sigma_j|\\hat{x}_i)\\right] $$\n",
    "\n",
    "$$ \\mu_j \\sim \\textrm{Uniform}(\\min=0.0,\\max=20) $$\n",
    "$$ \\sigma_j \\sim \\textrm{Uniform}(\\min=0.1,\\max=10) $$\n",
    "$$ [w_1,w_2] \\sim \\textrm{Dir}([w_1,w_2];[\\alpha_1=1,\\alpha_2=1]) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Exercise: Build a PyMC model with the variables above. With the following steps:\n",
    " - Start with the data. Data can be defined in a PyMC model using `pm.Data`.\n",
    " - Then add the model parameters and variables we have defined above.\n",
    " - Then, have a look at the [Mixture](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.Mixture.html) documentation and define your mixture components and likelihood.\n",
    " - It is always helpful to plot the plate notation of the model to see how our model is structured, based on what we did in our linear regression tutorial, plot a plate notation for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f882e22-f06e-4529-bc7c-7d08b68cb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "# just define the model and its components, no sampling or plotting needed here.\n",
    "\n",
    "with pm.Model() as mixture_model:\n",
    "    ## First define your data here:\n",
    "    x = \n",
    "\n",
    "    ## Now define your model variables\n",
    "    mu1 = \n",
    "    mu2 = \n",
    "    sigma1 = \n",
    "    sigma2 = \n",
    "    w = \n",
    "\n",
    "    ## Mixture components and likelihood\n",
    "    mixture_components = \n",
    "    mixture_likelihood = \n",
    "    \n",
    "## plate notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7570e8-9b4c-439e-a4be-58cd485b7517",
   "metadata": {},
   "source": [
    "### MCMC sampling\n",
    "\n",
    "Note that we are now actually performing MCMC (and not MC).\n",
    "\n",
    "#### Exercise: perform sampling with the model we defined above. Run your sampling with `discard_tuned_samples=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea2c62-6cb2-43fc-aeb7-78f1b466de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2603ea7-d951-410d-b7ae-e4bff970b5e1",
   "metadata": {},
   "source": [
    "#### Exercise: Use `az.plot_trace` to look at how your chains have \"walked\" (for all parameters). First do this for `posterior` (the default), then do the same for `warmup_posterior`. Do you notice anything unusual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc8085-437e-46fa-b4e0-acfc6efed8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "# Posterior\n",
    "\n",
    "# Warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e467cbb-7438-4638-9abc-0e6db2aefaf2",
   "metadata": {},
   "source": [
    "#### Exercise: There are a few complexities in our sampling that the plots above demonstrate. Can you unpack them, describe them and explain the underlying cause? To add more evidence for the complexity we are seeing, print summary of the posterior with `pm.summary`. Think about what the values in the summary table mean. Use 95% HDI probability for interval estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b653387-0d36-45bf-ba06-64ba0e3e8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83146ee-3c7d-4eb3-8cc3-e899ad28ab65",
   "metadata": {},
   "source": [
    "#### Exercise: If you have identified any issues with your model, redefine them blow and rerun the steps outline.\n",
    "\n",
    "Step 1: redefine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a28e-1ce2-4e51-853f-26b9ad5ceb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "# just define the model and its components, no sampling or plotting needed here.\n",
    "\n",
    "with pm.Model() as mixture_model:\n",
    "    ## First define your data here:\n",
    "    x = \n",
    "\n",
    "    ## Now define your model variables\n",
    "    mu1 = \n",
    "    mu2 = \n",
    "    sigma1 = \n",
    "    sigma2 = \n",
    "    w = \n",
    "\n",
    "    ## Mixture components and likelihood\n",
    "    mixture_components = \n",
    "    mixture_likelihood = \n",
    "    \n",
    "## plate notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce20e2-e959-4882-a9eb-6ff4aa073d59",
   "metadata": {},
   "source": [
    "Step 2: redo MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aa8d6-9207-48d5-9772-0482f4dbb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ab770-38c7-40d4-81e0-d63cae9baef4",
   "metadata": {},
   "source": [
    "step 3: replot posterior traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569afca-c86b-4251-b2a0-89b78d77a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "# Posterior\n",
    "\n",
    "# Warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fbeba-c9a0-45a1-a1f4-2c1f1d151df9",
   "metadata": {},
   "source": [
    "Step 4: Make the sample summary table again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bfe2e-077e-4092-9567-f76c68be9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba312f20-641a-48c5-8076-56f0848d9de4",
   "metadata": {},
   "source": [
    "Now that we are hopefully satisfied, let's explore the posterior results:\n",
    "\n",
    "#### Exercise: using plotting functions we have discussed in earlier exercises, plot the marginal posterior samples for our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f69dd8-0abf-43b7-89cd-9cac45f0de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650468f4-6ef4-434f-8721-8ac915eba1b4",
   "metadata": {},
   "source": [
    "#### Exercise: using plotting functions we have discussed in earlier exercises, plot the joint and margian posterior samples of all model parameters. Interpret what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abca6e3-f176-431a-acc5-80061cf15272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31066c5-c154-4df6-af1d-efd9a7298e1d",
   "metadata": {},
   "source": [
    "#### Exercise: Plot a model representing your posterior point estimates on top of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444d131-b564-4588-82f3-13ccd298873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "ax = sns.displot(DATA, x='x', kind='hist', bins=50, rug=True, aspect=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db12c5-efae-46df-adf5-2ae4b3a7c3c1",
   "metadata": {},
   "source": [
    "## You can now save the notebook and download it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5",
   "language": "python",
   "name": "pymc5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
