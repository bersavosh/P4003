{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03a4c1e-6229-4b50-93e3-ca7cb17f7faf",
   "metadata": {},
   "source": [
    "## Student Name: \n",
    "\n",
    "---\n",
    "\n",
    "# Tutorial 2 - Part II: Bayesian mixture modeling\n",
    "\n",
    "All the parts that require action (either in the code or equations) are flagged by `<your turn>` or $\\color{red}{<your~turn>}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321a791-1929-491b-9244-046d0b1b8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.color_palette(\"hls\", 8)\n",
    "\n",
    "# Setting matplotlib fonts\n",
    "from matplotlib import rc\n",
    "font = {'family': 'serif',\n",
    "        'weight': 'bold',\n",
    "        'size': '14'}\n",
    "rc('font', **font)\n",
    "\n",
    "\n",
    "# Below is a set of colors for matplotlib that is colorblind-friendly\n",
    "# To use them in plotting commands, you can simply set \"color=colorset[N]\",\n",
    "# where N is an integer in [0,16), reflecting the index of the colors below.\n",
    "colorset = ['#000000','#00270C','#00443C','#005083',\n",
    "            '#034BCA','#483CFC','#9C2BFF','#EB24F4',\n",
    "            '#FF2DC2','#FF4986','#FF7356','#FFA443',\n",
    "            '#EBD155','#D3F187','#D7FFC8','#FFFFFF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08ee47-0889-4a46-9690-95e822b3ecd8",
   "metadata": {},
   "source": [
    "## Mixture modeling\n",
    "\n",
    "Starting with the example in the lecture, we have made $N$ independent measurements of a quantity $Y$ (with each measurement containing normally-distributed uncertainties), at different values of quantity $X$. Assuming $Y$ as a function of $X$, we want to infer a model for the \"model\" $Y$ (which we label $Y_{\\rm{Model}}$), given what we can *observe* as $Y$ ($Y_{\\rm{obs}}$).\n",
    "\n",
    "### Data\n",
    "\n",
    "Our data consists of a sample a single quantity per measurement:\n",
    "\n",
    "\n",
    "$$ \\hat{X} = [\\hat{x}_1,\\cdots,\\hat{x}_N] $$\n",
    "\n",
    "\n",
    "In this unit we will use [Pandas](https://pandas.pydata.org/) for data operations. Here is [a quick cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) for using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4414bd4-0c6d-44f3-bc98-31a65c9e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('https://raw.githubusercontent.com/bersavosh/P4003/refs/heads/main/Tutorials/T2_data_mixture.csv')\n",
    "DATA.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805e4de-dcd0-4acd-81af-50a95af78b67",
   "metadata": {},
   "source": [
    "### EDA\n",
    "#### Exercise: Make a histogram and a kde plot the measurements using [Seaborn](https://seaborn.pydata.org/tutorial/distributions.html). Include a rug visualization in both. For the histogram, assign binning so that you have a total of 50 bins. For the KDE plot, set bandwidth to be 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f8120-ade2-403e-8f40-fb25a786f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "## Two plots should be produced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3888fe3-d3f1-4c9b-b017-203b26eb8885",
   "metadata": {},
   "source": [
    "### Our model\n",
    "\n",
    "Similar to the lecture, we consider that our observed values of X are random draws from a *mixture* of two gaussians:\n",
    "$$ \\hat{X}\\sim \\rm{Mixture}\\{\\mathcal{N}_1(\\mu_1, \\sigma_1),\\mathcal{N}_2(\\mu_2, \\sigma_2)\\} $$\n",
    "\n",
    "\n",
    "So:\n",
    "\n",
    "$$ p(\\hat{X} |\\mu_1,\\sigma_1,w_1,\\mu_2,\\sigma_2,w_2) = \\prod_{i=1}^{N}\\left[\\sum_{j=1}^{2} w_j \\mathcal{N}(\\mu_j,\\sigma_j|\\hat{x}_i)\\right] $$\n",
    "\n",
    "$$ \\mu_j \\sim \\textrm{Uniform}(\\min=0.0,\\max=20) $$\n",
    "$$ \\sigma_j \\sim \\textrm{Uniform}(\\min=0.1,\\max=10) $$\n",
    "$$ [w_1,w_2] \\sim \\textrm{Dir}([w_1,w_2];[\\alpha_1=1,\\alpha_2=1]) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Exercise: Build a PyMC model with the variables above. With the following steps:\n",
    " - Start with the data. Data can be defined in a PyMC model using `pm.Data`.\n",
    " - Then add the model parameters and variables we have defined above.\n",
    " - Then, have a look at the [Mixture](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.Mixture.html) documentation and define your mixture components and likelihood.\n",
    " - It is always helpful to plot the plate notation of the model to see how our model is structured, based on what we did in our linear regression tutorial, plot a plate notation for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f882e22-f06e-4529-bc7c-7d08b68cb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "# define the model and its components, and visualize the plate notation\n",
    "\n",
    "with pm.Model() as mixture_model:\n",
    "    ## First define your data here:\n",
    "    x = \n",
    "\n",
    "    ## Now define your model variables\n",
    "    mu1 = \n",
    "    mu2 = \n",
    "    sigma1 = \n",
    "    sigma2 = \n",
    "\n",
    "    w = \n",
    "\n",
    "    ## Mixture components and likelihood\n",
    "    mixture_components = \n",
    "\n",
    "    mixture_likelihood = \n",
    "\n",
    "## plate notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf6e5b3-591c-4836-9dc9-c151ec63ef03",
   "metadata": {},
   "source": [
    "### Prior and prior-predictive sampling\n",
    "\n",
    "#### Exercise: with the model defined above, perform prior and prior-predictive sampling with 1000 draws and save the output as an inference data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17526a-ab6d-4ce0-8b5e-5204c08f1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "with mixture_model:\n",
    "    mixture_mcmc_sample_prior_predictive = \n",
    "\n",
    "mixture_mcmc_sample_prior_predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919e291-c76d-438f-a480-94ab9fb1d3ca",
   "metadata": {},
   "source": [
    "#### Exercise: plot marginal and pair-wise distribution of prior samples. You can use the commands we explored in previous workshop, passing the inference data and indicating `group='prior'`. Explore if the results are as expected based on our model definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128ceb4-3b51-40f5-aae7-660028994fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "## Two plots should be produced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b47f55-26fc-4371-af1f-1ad6b8da1788",
   "metadata": {},
   "source": [
    "#### Exercise: plot empirical cumulative distribuion function of the data with the functionalities defined in [Arviz](https://python.arviz.org/en/stable/examples/index.html). Assign the output figure object to python variable `ax_cdf`.\n",
    "**Hint**: you can use the pandas data frame directly which contains the data, or use the `constant_data` group in your inference data object, and extract the values using `to_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71205884-af04-4b46-beab-b9174f62ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "ax_cdf = \n",
    "\n",
    "ax_cdf.legend()\n",
    "ax_cdf.set_xlabel('x')\n",
    "ax_cdf.set_ylabel('Proportion');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d7b2a-c2fe-4ddd-8248-e5e18ed46079",
   "metadata": {},
   "source": [
    "#### Exercise: Repeat the step above, and plot ECDF of the prior predictive sample on top of the ECDF of the data.\n",
    "\n",
    "**Hint**: you can pass `ax=ax_cdf` in the subsequent plotting commands for plottings to occur on the same canvas as where you plot your data. \n",
    "\n",
    "**Hint**: Our case here is a bit challenging, how would we go about visualizing more than one set of our prior samples? First have a look at navigating subsets within inference data sets. E.g., look at the functionality of `sel` in the [documentation](https://python.arviz.org/en/stable/getting_started/WorkingWithInferenceData.html#get-a-subset-of-chains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167592b5-2ef5-4c94-a016-2fc3ae2af129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "## add as many commands and definitions as you need in this cell\n",
    "\n",
    "ax_cdf = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax_cdf.legend()\n",
    "ax_cdf.set_xlabel('x')\n",
    "ax_cdf.set_ylabel('Proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285ef7b-4fda-4291-b5c9-30adfa73eae2",
   "metadata": {},
   "source": [
    "#### Exercise: Similar to above, make KDE plots of the prior predictive samples on top of the KDE of the data. For this exercise, don't use Seaborn and instead use `plot_kde` in arviz. Use a bandwidth of 0.25 for all KDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed2fd1-97b1-43ad-8242-0491c465be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "## add as many commands and definitions as you need in this cell\n",
    "\n",
    "ax_kde = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax_kde.legend()\n",
    "ax_kde.set_xlabel('x')\n",
    "ax_kde.set_ylabel('p(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7570e8-9b4c-439e-a4be-58cd485b7517",
   "metadata": {},
   "source": [
    "### Sampling from the posterior\n",
    "\n",
    "#### Exercise: perform sampling from the posterior with the model we defined above with 2000 draws post warm-up (\"tuning\") on 4 chains, and warm-up sample size of 1000. Use `discard_tuned_samples=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea2c62-6cb2-43fc-aeb7-78f1b466de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "with mixture_model:\n",
    "    mixture_mcmc_sample = \n",
    "\n",
    "mixture_mcmc_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9aa057-81c4-406c-91a6-1f80e89b5f6b",
   "metadata": {},
   "source": [
    "#### Exercise: Print a summary table of the posterior samples with methods we explored last week. Think about what the values in the summary table mean. Use 95% HDI probability for interval estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51054735-55f9-4d96-bcfd-959b003752ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "fit_summary = \n",
    "\n",
    "fit_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2603ea7-d951-410d-b7ae-e4bff970b5e1",
   "metadata": {},
   "source": [
    "#### Exercise: Is everything looking reasonable in the table you produced? Plot the \"traces\" of your chains and look at how your chains have stepped through the parameter space (for all parameters). First do this for `warmup_posterior`, then for `posterior` (the default). Do you notice anything unusual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc8085-437e-46fa-b4e0-acfc6efed8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "# Warmup trace\n",
    "\n",
    "\n",
    "\n",
    "# post-warm-up trace\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2c437-b78f-42fb-82bd-1fe7c19bd864",
   "metadata": {},
   "source": [
    "### Workflow loop of modification\n",
    "\n",
    "If you have identified any issues with your model, redefine them blow and rerun the steps outline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83146ee-3c7d-4eb3-8cc3-e899ad28ab65",
   "metadata": {},
   "source": [
    "#### Exercise - Step 1: redefine your statistical model with the potential improvement and print plate notation again. Note that we are using a new name for our new model `updated_mixture_model` compared to what we called `mixture_model` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a28e-1ce2-4e51-853f-26b9ad5ceb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "# just define the model and its components, no sampling or plotting needed here.\n",
    "\n",
    "with pm.Model() as updated_mixture_model:\n",
    "    ## First define your data here:\n",
    "    x = \n",
    "\n",
    "    ## Now define your model variables\n",
    "    mu1 = \n",
    "    mu2 = \n",
    "    sigma1 = \n",
    "    sigma2 = \n",
    "\n",
    "    w = \n",
    "\n",
    "    ## Mixture components and likelihood\n",
    "    mixture_components = \n",
    "\n",
    "    mixture_likelihood = \n",
    "\n",
    "\n",
    "## plate notation\n",
    "pm.model_to_graphviz(updated_mixture_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce20e2-e959-4882-a9eb-6ff4aa073d59",
   "metadata": {},
   "source": [
    "#### Exercise - Step 2: perform prior predictive sampling with the new model with 1000 draws and plot pair-wise plot of the posteriors and include marginal distributions. Compare the results with what we had in our previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8114795-1d19-4ff7-b784-a4f6dc0a5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "with updated_mixture_model:\n",
    "    updated_mixture_mcmc_sample_prior_predictive = \n",
    "\n",
    "## Pairwise plot:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38682d-1973-4198-8e75-2da971437bcf",
   "metadata": {},
   "source": [
    "You can plot the ECDFs as well, but we leave that for now.\n",
    "\n",
    "#### Exercise - Step 3: perform sampling from the posterior with the updated model we defined above with 2000 draws post warm-up (\"tuning\") on 4 chains, and warm-up sample size of 1000. Use `discard_tuned_samples=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8aa8d6-9207-48d5-9772-0482f4dbb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "with updated_mixture_model:\n",
    "    updated_mixture_mcmc_sample = \n",
    "\n",
    "updated_mixture_mcmc_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a500181-889e-44a0-9000-48b6ad1f8ba7",
   "metadata": {},
   "source": [
    "#### Exercise - step 4: Print a summary table of the posterior sample from our updated model. Use 95% HDI probability for interval estimation. Do the values look promising? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729462a-f320-4772-a1b2-53b951cbe84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "updated_fit_summary = \n",
    "updated_fit_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ab770-38c7-40d4-81e0-d63cae9baef4",
   "metadata": {},
   "source": [
    "#### Exercise - step 5: plot the sampling traces for both warm up and post-warm-up samples. Assess the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569afca-c86b-4251-b2a0-89b78d77a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "# Warmup trace\n",
    "\n",
    "\n",
    "\n",
    "# post-warm-up trace\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93305-e466-49dc-800e-457112efb4ab",
   "metadata": {},
   "source": [
    "### Inspecting posteriors and influence of priors\n",
    "\n",
    "#### Exercise: Make a pair-wise plot of the posterior samples (including marginal KDEs). Do they look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35684c76-eacd-4ba7-a40e-f1532b587583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b36702-9afb-4d46-a421-cb62d4180f48",
   "metadata": {},
   "source": [
    "#### Exercise: In the cell below, we append the prior samples we generated at the beginning to our new `updated_mixture_mcmc_sample` inference data. Using this, generate plots comparing priors to posteriors for model parameters and evaluate the influence of priors. For this exercise, make two plots: 1- distribution comparison with `plot_dist_comparison`, 2- use pair-wise plot, overlaying both priors and posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f091b-b6a7-4254-92e2-dfc1fa920bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No action needed in this cell, just run it once!\n",
    "\n",
    "updated_mixture_mcmc_sample.extend(mixture_mcmc_sample_prior_predictive)\n",
    "updated_mixture_mcmc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1d8b3-5f5f-4ffa-92f7-ae5f7470855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "## distribution comparison plot\n",
    "\n",
    "\n",
    "\n",
    "## Pair-wise comparison plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fbeba-c9a0-45a1-a1f4-2c1f1d151df9",
   "metadata": {},
   "source": [
    "### Posterior predictive sampling\n",
    "\n",
    "#### Exercise: perform posterior predictive sampling (make sure to use `extend_inferencedata=True`), and use the same method as we visualized the prior predictive sample for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bfe2e-077e-4092-9567-f76c68be9c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>: cell 1 - sampling\n",
    "\n",
    "with updated_mixture_model:\n",
    "    # sampling posterior predictive and extending the updated_mixture_mcmc_sample\n",
    "    \n",
    "\n",
    "# If done correctly, you should see the group in the inference data\n",
    "updated_mixture_mcmc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b7913-4693-42aa-85a6-f64ff9bf644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>: cell 2 - plotting CDF\n",
    "\n",
    "ax_cdf = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax_cdf.legend()\n",
    "ax_cdf.set_xlabel('x')\n",
    "ax_cdf.set_ylabel('Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ecf3e-bc6e-4dd6-963b-a28db16ee3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>: cell 3 - plotting KDE\n",
    "\n",
    "ax_kde = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax_kde.legend()\n",
    "ax_kde.set_xlabel('x')\n",
    "ax_kde.set_ylabel('p(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba312f20-641a-48c5-8076-56f0848d9de4",
   "metadata": {},
   "source": [
    "Now that we are hopefully satisfied, let's explore the posterior results:\n",
    "\n",
    "#### Exercise: using plotting functions we have discussed in earlier exercises, plot the marginal posterior samples for our parameters with poist and interval estimates plotted on top. Use median for point estimate and 95% HDI for interval estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f69dd8-0abf-43b7-89cd-9cac45f0de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <your turn>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db12c5-efae-46df-adf5-2ae4b3a7c3c1",
   "metadata": {},
   "source": [
    "## You can now save the notebook and download it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5",
   "language": "python",
   "name": "pymc5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
